{"cells":[{"metadata":{},"cell_type":"markdown","source":"### This notebook has the following Notebooks as reference on detailed analysis and reasonable way of handling the missing values, and feature generation and selection. Apart from that there are few other good notebooks from where this notebook got some value addition pieces of ideas! I wish to thank my fellow kagglers who compel me to learn and grow!\n\n### [Kaggle Notebook] [Jane TF Keras LSTM](https://www.kaggle.com/rajkumarl/jane-tf-keras-lstm) (to fill missing values)\n### [Kaggle Notebook] [Jane Day 242 Feature Generation and Selection](https://www.kaggle.com/rajkumarl/jane-day-242-feature-generation-and-selection) (to generate and select features)\n"},{"metadata":{},"cell_type":"markdown","source":"### Training part of this notebook is available in the following notebook. Three models were trained with three-folds of data. Weights of those models are saved in h5 format to be used in this notebook for inference.\n\n### [Kaggle Notebook] [TF Residual Network on Select Features](https://www.kaggle.com/rajkumarl/tf-residual-network-on-select-features-training) (for model training)"},{"metadata":{},"cell_type":"markdown","source":"# 1. IMPORT LIBRARIES"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import models, layers, regularizers\nimport datatable\nimport warnings\n# ignore warnings during notebook running\nwarnings.filterwarnings('ignore')\nSEED = 2222\n# set seed\ntf.random.set_seed(SEED)\nnp.random.seed(SEED)","execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# 2. LOAD DATA AND OPTIMIZE MEMORY"},{"metadata":{"trusted":true},"cell_type":"code","source":"# path of train data file\ntrain_path = '../input/jane-street-market-prediction/train.csv'\n\n# use datatable to load big data file\ntrain_file = datatable.fread(train_path).to_pandas()\ntrain_file.info()","execution_count":2,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2390491 entries, 0 to 2390490\nColumns: 138 entries, date to ts_id\ndtypes: float64(135), int32(3)\nmemory usage: 2.4 GB\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It is found from info() that there are only two datatypes - float64 and int32\n# try to convert to low-memory-space data types by comparing max and min values of data\n# with the preset max and min values of low-memory-space data types\nfor c in train_file.columns:\n    min_val, max_val = train_file[c].min(), train_file[c].max()\n    if train_file[c].dtype == 'float64':\n        if min_val>np.finfo(np.float16).min and max_val<np.finfo(np.float16).max:\n            train_file[c] = train_file[c].astype(np.float16)\n        elif min_val>np.finfo(np.float32).min and max_val<np.finfo(np.float32).max:\n            train_file[c] = train_file[c].astype(np.float32)\n    elif train_file[c].dtype == 'int32':\n        if min_val>np.iinfo(np.int8).min and max_val<np.iinfo(np.int8).max:\n            train_file[c] = train_file[c].astype(np.int8)\n        elif min_val>np.iinfo(np.int16).min and max_val<np.iinfo(np.int16).max:\n            train_file[c] = train_file[c].astype(np.int16)\ntrain_file.info()","execution_count":3,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2390491 entries, 0 to 2390490\nColumns: 138 entries, date to ts_id\ndtypes: float16(135), int16(1), int32(1), int8(1)\nmemory usage: 631.5 MB\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### That's a great reduction in memory usage (around 74% reduction)! It will help us go further efficiently!"},{"metadata":{},"cell_type":"markdown","source":"# 3. HANDLING MISSING VALUES"},{"metadata":{"trusted":true},"cell_type":"code","source":"# take useful features only...\nfeatures = train_file.columns[train_file.columns.str.contains('feature')]\n# find range of values\nval_range = train_file[features].max()-train_file[features].min()\n# filler value if lesser by minimum value by 1% of range\nfiller = pd.Series(train_file[features].min()-0.01*val_range, index=features)\n# This filler value will be used as a constant replacement of missing values \n\n\n\"\"\"\nA function to fill all missing values with negative outliers as discussed in the referred notebook\nhttps://www.kaggle.com/rajkumarl/jane-tf-keras-lstm\n\"\"\"\ndef fill_missing(df):\n    df[features] = df[features].fillna(filler)\n    return df  \n\ntrain = fill_missing(train_file)\ntrain = train.loc[train.weight > 0]\ntrain.info()","execution_count":4,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1981287 entries, 1 to 2390489\nColumns: 138 entries, date to ts_id\ndtypes: float16(135), int16(1), int32(1), int8(1)\nmemory usage: 538.5 MB\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Now we have %d missing values in our data\" %train.isnull().sum().sum())","execution_count":5,"outputs":[{"output_type":"stream","text":"Now we have 0 missing values in our data\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 4. FEATURE GENERATION AND SELECTION"},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nfrom notebook\nhttps://www.kaggle.com/rajkumarl/jane-day-242-feature-generation-and-selection/comments\n\"\"\"\ndef feature_transforms(df):\n    # Generate Features using Linear shifting, Natural Logarithm and Square Root\n    for f in [f'feature_{i}' for i in range(1,130)]: \n        # linear shifting to value above 1.0\n        df['pos_'+str(f)] = (df[f]+abs(train[f].min())+1).astype(np.float16)\n    for f in [f'feature_{i}' for i in range(1,130)]: \n        # Natural log of all the values\n        df['log_'+str(f)] = np.log(df['pos_'+str(f)]).astype(np.float16)\n    for f in [f'feature_{i}' for i in range(1,130)]: \n        # Square root of all the values\n        df['sqrt_'+str(f)] = np.sqrt(df['pos_'+str(f)]).astype(np.float16)\n    \n    # Linearly shifted values are used for log and sqrt transformations\n    # However they are useless since we have our original values which are 100% correlated\n    # Let's drop them from our data\n    df.drop([f'pos_feature_{i}' for i in range(1,130)], inplace=True, axis=1)\n    \n    # From the Shap Dependence plots, the following features seem to have cubic relationship with target\n    cubic = [37, 39, 67, 68, 89, 98, 99, 118, 119, 121, 124, 125, 127]\n    for i in cubic:\n        f = f'feature_{i}'\n        threes = np.array([3])\n        df['cub_'+f] =np.power(df[f], threes) \n        \n    # From the Shap Dependence plots, the following features seem to have quadratic relationship with target\n    quad = [6, 37, 39, 40, 53, 60, 61, 62, 63, 64, 67, 68, 89, 98, 99, 101, 113, 116, 118, 119, 121, 123, 124, 125, 127]\n    for i in quad:\n        f = f'feature_{i}'\n        df['quad_'+f] =np.square(df[f]) \n    \n    return df","execution_count":56,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nfrom notebook\nhttps://www.kaggle.com/rajkumarl/jane-day-242-feature-generation-and-selection/comments\n\"\"\"\ndef manipulate_pairs(df):\n    # features that can be added together or subtracted\n    add_pairs = [(3,6), (15,26), (19,26), (30,37), (34,33), (35,39),(94,65), (101,4)]\n    for i,j in add_pairs:\n        df[f'add_{i}_{j}'] = df[f'feature_{i}']+df[f'feature_{j}']\n        df[f'sub_{i}_{j}'] = df[f'feature_{i}']-df[f'feature_{j}']\n\n    add_log_pairs = [(9,20), (22,37), (28,39), (29,25), (65,91), (74,103),(99,126), (109,7), (111,87), (112,97), (118,112)]\n    for i,j in add_log_pairs:\n        df[f'add_{i}_log{j}'] = df[f'feature_{i}']+df[f'log_feature_{j}']\n        df[f'sub_{i}_log{j}'] = df[f'feature_{i}']-df[f'log_feature_{j}']\n    # features that can be multiplied together\n    mul_pairs = [(5,42), (12,66), (37,45), (39,95), (122,35)]\n    for i,j in mul_pairs:\n        df[f'mul_{i}_{j}'] = df[f'feature_{i}']*df[f'feature_{j}']\n\n    mul_log_pairs = [(5,42), (6,42), (11,99), (21,42), (81,66), (98,20), (122,35)]\n    for i,j in mul_log_pairs:\n        df[f'mul_{i}_log{j}'] = df[f'feature_{i}']*df[f'log_feature_{j}']\n    return df","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\"\nfrom notebook\nhttps://www.kaggle.com/rajkumarl/jane-day-242-feature-generation-and-selection/comments\n\"\"\"\nselected_features = ['weight', 'feature_1', 'feature_2', 'feature_6', 'feature_9',\n       'feature_10', 'feature_16', 'feature_20', 'feature_29', 'feature_37',\n       'feature_38', 'feature_39', 'feature_40', 'feature_51', 'feature_52',\n       'feature_53', 'feature_54', 'feature_69', 'feature_70', 'feature_71',\n       'feature_83', 'feature_100', 'feature_109', 'feature_112',\n       'feature_122', 'feature_123', 'feature_124', 'feature_126',\n       'feature_128', 'feature_129', 'log_feature_1', 'log_feature_2',\n       'log_feature_6', 'log_feature_37', 'log_feature_38', 'log_feature_39',\n       'log_feature_40', 'log_feature_50', 'log_feature_51', 'log_feature_52',\n       'log_feature_53', 'log_feature_54', 'log_feature_69', 'log_feature_70',\n       'log_feature_71', 'log_feature_109', 'log_feature_112',\n       'log_feature_122', 'log_feature_123', 'log_feature_126',\n       'log_feature_128', 'log_feature_129', 'sqrt_feature_1',\n       'sqrt_feature_2', 'sqrt_feature_6', 'sqrt_feature_9', \n       'sqrt_feature_10', 'sqrt_feature_37', 'sqrt_feature_38', 'sqrt_feature_39',\n       'sqrt_feature_40', 'sqrt_feature_50', 'sqrt_feature_51',\n       'sqrt_feature_52', 'sqrt_feature_53', 'sqrt_feature_54',\n       'sqrt_feature_56', 'sqrt_feature_69', 'sqrt_feature_70',\n       'sqrt_feature_71', 'sqrt_feature_83', 'sqrt_feature_109',\n       'sqrt_feature_112', 'sqrt_feature_122', 'sqrt_feature_123',\n       'sqrt_feature_124', 'sqrt_feature_126', 'sqrt_feature_128',\n       'sqrt_feature_129', 'cub_feature_37', 'cub_feature_39',\n       'quad_feature_53', 'quad_feature_64', 'quad_feature_67',\n       'quad_feature_68', 'sub_3_6', 'sub_30_37', 'add_35_39', 'add_9_log20',\n       'sub_9_log20', 'add_29_log25', 'sub_29_log25', 'add_109_log7',\n       'sub_109_log7', 'add_112_log97', 'sub_112_log97', 'mul_39_95',\n       'mul_122_35', 'mul_6_log42', 'mul_122_log35']","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. RESIDUAL NETWORK MODELING"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Residual(tf.keras.Model):  \n    \"\"\"The Residual layer of ResNet\"\"\"\n    def __init__(self, units):\n        super().__init__()\n        # initialize necessary dense and batch norm layers\n        self.d1 = layers.Dense(units, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))\n        self.d2 = layers.Dense(units, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))\n        self.d3 = layers.Dense(units, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4))\n        self.bn1 = layers.BatchNormalization()\n        self.bn2 = layers.BatchNormalization()\n\n    def call(self, X):\n        # stack two dense layers in series...\n        Y = tf.keras.activations.relu(self.bn1(self.d1(X)))\n        Y = layers.Dropout(0.3)(self.bn2(self.d2(Y)))\n        # ... and concatenate them with a third dense layer \n        X = self.d3(X)\n        Y += X\n        # apply dropout to avoid overfitting\n        return layers.Dropout(0.3)(tf.keras.activations.relu(Y))","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ResnetBlock(layers.Layer):\n    def __init__(self, num_units, num_residuals, **kwargs):\n        super(ResnetBlock, self).__init__(**kwargs)\n        # initialize a list of layers\n        self.residual_layers = []\n        for i in range(num_residuals):\n            # append list with residual layers\n            self.residual_layers.append(Residual(num_units))\n\n    def call(self, X):\n        for layer in self.residual_layers.layers:\n            # stack residual layers in series\n            X = layer(X)\n        return X","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    # a keras Sequential model\n    model= tf.keras.Sequential([\n        # model receives data with 100 features\n        layers.Input(shape=(100,)),\n        # incorporate noise to avoid overfitting\n        layers.GaussianNoise(0.2),\n        # introduce first layer before ResNet blocks with regularizers and relu activation\n        layers.Dense(64, kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)),\n        layers.BatchNormalization(),\n        layers.Activation('relu'),\n        # a dropout layer to avoid overfitting\n        layers.Dropout(0.5),\n        # four subsequent ResNet blocks\n        ResnetBlock(64, 2),\n        ResnetBlock(128, 2),\n        ResnetBlock(256, 2),\n        ResnetBlock(512, 2),\n        # two layers after ResNet blocks\n        layers.Dense(64, activation='relu'),\n        # output layer - binary classification - sigmoid activation\n        layers.Dense(1, activation='sigmoid')])\n    \n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n                  loss=tf.keras.losses.BinaryCrossentropy(), \n                  metrics=['accuracy'])\n    return model\n","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# find out the files in the training data path\n!ls '../input/tf-residual-network-on-select-features-training'","execution_count":13,"outputs":[{"output_type":"stream","text":"__notebook__.ipynb  __results___files\t\tresnet_select_feature_2.h5\r\n__output__.json     custom.css\t\t\tresnet_select_feature_3.h5\r\n__results__.html    resnet_select_feature_1.h5\r\n","name":"stdout"}]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# path of our saved model weihts\nPATH = '../input/tf-residual-network-on-select-features-training/'\nmodels = []\nfolds = 3\nfor i in range(folds):\n    model = create_model()\n    model.load_weights(PATH+f'resnet_select_feature_{i+1}.h5')\n    models.append(model)\nprint('Modeling phase completed')","execution_count":16,"outputs":[{"output_type":"stream","text":"Modeling phase completed\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 6. INFERENCE"},{"metadata":{"trusted":true},"cell_type":"code","source":"# test your code with available test file\nt_file = pd.read_csv('../input/jane-street-market-prediction/example_test.csv')\n# Sample one row\ntest = t_file.loc[1]\n# fill missing values, if any\nif test[features].isna().any().sum():\n    test[features] = fill_missing(test[features])\n# convert dimensions of data as model expects\n# (100,) to (1,100)\ntest = np.expand_dims(test,-2)\n# feature generation\ntest = feature_transforms(test)\ntest = manipulate_pairs(test)\n# feature selection\ntest = test[selected_features]\n# convert to model supported dtype\ntest = np.array(test, dtype=np.float)\n# predict target\naction = np.mean([model(test).numpy() for model in models])\n# binary classification\n# set threshold as 0.5\naction = 1 if action>0.5 else 0\n# output prediction\naction","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm.auto import tqdm\nimport janestreet\njanestreet.make_env.__called__ = False\nenv = janestreet.make_env()\nfor test,pred in tqdm(env.iter_test()):\n    if test.weight.item()==0:\n        pred.action = 0\n    else:\n        if test[features].isna().any().sum():\n            test[features] = fill_missing(test[features])\n        test = feature_transforms(test)\n        test = manipulate_pairs(test)\n        test = np.array(test[selected_features], dtype=np.float)\n        action = np.mean([model(test).numpy() for model in models])\n        pred.action = 1 if action>0.5 else 0\n    env.predict(pred)","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"|          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e71a31e73da746f2be64d74410d953a6"}},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Thank you for your time!"}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}